# ğŸš€ LLM Observability Platform

> Production-ready observability platform for LLM-powered applications and AI agents

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Go Version](https://img.shields.io/badge/Go-1.25+-00ADD8?logo=go)](https://go.dev)

## ğŸ“‹ Overview

Comprehensive monitoring and debugging platform for AI applications. Track costs, performance, and reliability across multiple LLM providers.

## âœ¨ Features

- ğŸ” **Debug Faster** - Trace every LLM call with full context
- ğŸ’° **Optimize Costs** - Identify expensive calls and caching opportunities
- âš ï¸ **Prevent Failures** - Real-time alerting for issues
- ğŸ“Š **Understand Behavior** - Semantic search across all prompts

### Supported Providers
- âœ… OpenAI (GPT-4, GPT-3.5, etc.)
- âœ… Anthropic (Claude 3.5, Claude 3, etc.)
- âœ… Cohere
- âœ… Custom/Self-hosted models

## ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Client Applications                â”‚
â”‚   (Python/JS/Go SDKs integrated)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ HTTPS/gRPC
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         API Gateway (Kong)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Ingestion â”‚      â”‚ Query Serviceâ”‚
â”‚ Service  â”‚      â”‚    (Go)      â”‚
â”‚  (Go)    â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Message Queue (Kafka)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ClickHouseâ”‚â”‚ Redis  â”‚â”‚Prometheuâ”‚
â”‚(Storage)â”‚â”‚(Cache) â”‚â”‚(Metrics)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## ğŸš€ Quick Start

### Prerequisites
- Docker Desktop (v24+)
- Go (v1.21+)
- Node.js (v18+)
- Python (v3.11+)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/sahared/llm-observability.git
cd llm-observability
```

2. **Start infrastructure**
```bash
cd infrastructure
docker-compose up -d
```

3. **Verify services**
```bash
cd ../scripts
./health-check.sh
```

4. **Start backend**
```bash
cd ../backend
cp .env.example .env
go run cmd/api/main.go
```

5. **Start frontend**
```bash
cd ../frontend
npm install
npm run dev
```

### Access Points
- Frontend: http://localhost:3000
- Backend API: http://localhost:8080
- Grafana: http://localhost:3001 (admin/admin)

## ğŸ› ï¸ Technology Stack

**Backend:** Go, Fiber, ClickHouse, Kafka, Redis  
**Frontend:** React, TypeScript, Tailwind CSS  
**Infrastructure:** Kubernetes, Terraform, Prometheus, Grafana

## ğŸ“š Documentation

- [Architecture](docs/architecture.md)
- [API Reference](docs/api.md)
- [SDK Documentation](sdk/README.md)

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md).

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file.

## ğŸ“ Support

- ğŸ› Issues: [GitHub Issues](https://github.com/sahared/llm-observability/issues)
- ğŸ“§ Email: support@example.com

---

**Made with â¤ï¸ for the AI community**

â­ Star us on GitHub!